{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control using monte carlo:\n",
    "\n",
    "### Variables\n",
    "\n",
    "**__Value Function__** <br>\n",
    "__Q__ ==> Keeps track of value of each state/action pair, it is a Matrix of size 10x21x2 <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;10 -- No. of possible states of dealer. (one card is face up)<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;21 -- No. of possible states of player <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;2  -- No. of possible actions <br>\n",
    "\n",
    "*__Step Size__* \n",
    "\n",
    "$\\alpha = \\frac{1}{N(s,a)}$ <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;N(s,a)   ==> Number of times 'a' was taken when in state 's'.  <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|N(s,a)| ==> 10x21x2  <br>\n",
    "\n",
    "$\\epsilon = \\frac{N_0}{N_0 - N(s_t)}$\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$N_0$     ==> Constant value ~100 <br> \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;N($s_t$)  ==> Number of times state $s_t$ has been visited.\n",
    "\n",
    "### *Action Function*\n",
    "\n",
    "__Parameters__: state \n",
    "\n",
    "Chooses action based on Epsilon value:\n",
    "With $\\epsilon$ probability it takes a random action i.e hit or stick (prob of picking either action randomly is 0.5. <br>\n",
    "With 1-$\\epsilon$ probability we pick action greedily i.e action corresponding to max Q value. <br>\n",
    "\n",
    "\n",
    "### *Training Agent*\n",
    "\n",
    "__Parameters__: iterations\n",
    "\n",
    "Initialize start state.\n",
    "\n",
    "For each episode :<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Keep track of state,action pair, reward and number of times s,a pair were visited.<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; For each state action pair update Q table. <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $error = r - Q_prev $<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $Q =: Q + error*stepsize$ <br>\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_Agent:\n",
    "    def __init__(self,env,n0):\n",
    "        self.value = 0\n",
    "        self.n0 = float(n0)\n",
    "        self.env = env\n",
    "        self.N = np.zeros((env.dealer_values_count, env.player_values_count, env.actions_count))\n",
    "        self.Q = np.zeros((env.dealer_values_count, env.player_values_count, env.actions_count))\n",
    "        self.V = np.zeros((env.dealer_values_count, env.player_values_count))\n",
    "         \n",
    "       \n",
    "        \n",
    "        self.count_wins = 0\n",
    "        self.iterations = 0\n",
    "        \n",
    "    def get_action(self,state):\n",
    "        dealer_idx = state.dealer - 1\n",
    "        player_idx = state.player - 1\n",
    "        \n",
    "        n_visits = sum((self.N[dealer_idx, player_idx,:]))\n",
    "        epsilon = self.n0/(self.n0 + n_visits)\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            action = Actions.hit if random.random()<0.5 else Actions.stick\n",
    "            return action\n",
    "        else:\n",
    "            action = Actions.to_action(np.argmax(self.Q[dealer_idx,player_idx,:]))\n",
    "\n",
    "            return action\n",
    "    \n",
    "    def train(self, iterations):\n",
    "        \n",
    "        episode_pairs = []    \n",
    "        for epsiode in range(iterations):\n",
    "            s = self.env.get_start_state()\n",
    "            while not s.term:\n",
    "                a = self.get_action(s)\n",
    "                episode_pairs.append((s,a))\n",
    "                self.N[s.dealer-1,s.player-1,Actions.as_int(a)] += 1\n",
    "                \n",
    "                s,r = self.env.step(s, a)\n",
    "                \n",
    "            for curr_s,curr_a in episode_pairs:\n",
    "                dealer_idx = curr_s.dealer - 1\n",
    "                player_idx = curr_s.player - 1\n",
    "                action_idx = Actions.as_int(curr_a)\n",
    "                \n",
    "                step    =  1/self.N[dealer_idx, player_idx, action_idx]\n",
    "                error   =  r - self.Q[dealer_idx, player_idx, action_idx]\n",
    "                self.Q +=  step*error\n",
    "                \n",
    "        self.iterations += iterations\n",
    "        print (float(self.count_wins)/self.iterations*100)\n",
    "\n",
    "        # Derive value function\n",
    "        for d in range(self.env.dealer_values_count):\n",
    "            for p in range(self.env.player_values_count):\n",
    "                self.V[d,p] = max(self.Q[d, p, :])\n",
    "                \n",
    "    def plot_frame(self, ax):\n",
    "        def get_stat_val(x, y):\n",
    "            return self.V[x, y]\n",
    "\n",
    "        X = np.arange(0, self.env.dealer_values_count, 1)\n",
    "        Y = np.arange(0, self.env.player_values_count, 1)\n",
    "        X, Y = np.meshgrid(X, Y)\n",
    "        Z = get_stat_val(X, Y)\n",
    "        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "        return surf\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n",
      "1\n",
      "0.0\n",
      "2\n",
      "0.0\n",
      "3\n",
      "0.0\n",
      "4\n",
      "0.0\n",
      "5\n",
      "0.0\n",
      "6\n",
      "0.0\n",
      "7\n",
      "0.0\n",
      "8\n",
      "0.0\n",
      "9\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "n0 = 100\n",
    "agent = MC_Agent(Environment(), n0)\n",
    "for i in range (10):\n",
    "    print(i)\n",
    "    agent.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
